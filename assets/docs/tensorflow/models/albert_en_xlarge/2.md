# Module tensorflow/albert_en_xlarge/2
ALBERT: A Lite BERT for Self-supervised Learning of Language Representations

<!-- asset-path: legacy -->
<!-- dataset: Wikipedia and BooksCorpus -->
<!-- fine-tunable: true -->
<!-- format: saved_model_2 -->
<!-- language: en -->
<!-- module-type: text-embedding -->
<!-- network-architecture: Transformer -->


## Overview

ALBERT is "A Lite" version of BERT with greatly reduced number of parameters. It
was originally published by

*   Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma,
    Radu Soricut. [ALBERT: A Lite BERT for Self-supervised Learning of Language
    Representations](https://arxiv.org/abs/1909.11942). arXiv preprint
    arXiv:1909.11942, 2019.

This TF Hub model uses the implementation of ALBERT from the
TensorFlow Models repository on GitHub at
[tensorflow/models/official/nlp/modeling/networks/albert_encoder.py](https://github.com/tensorflow/models/blob/master/official/nlp/modeling/networks/albert_encoder.py).
It uses L=24 hidden layers (i.e., Transformer blocks),
a hidden size of H=2048,
and A=32 attention heads.

The weights of this TF2 SavedModel have been converted from
[albert_xlarge](https://tfhub.dev/google/albert_xlarge/3)
in TF1 Hub module format.

All parameters in the module are trainable, and fine-tuning all parameters is
the recommended practice.


## Usage

This SavedModel implements the encoder API for [text embeddings with transformer
encoders](https://www.tensorflow.org/hub/common_saved_model_apis/text#transformer-encoders).
It expects a dict with three int32 Tensors as input:
`input_word_ids`, `input_mask`, and `input_type_ids`.

The separate **preprocessor** SavedModel at
[http://tfhub.dev/tensorflow/albert_en_preprocess/2](http://tfhub.dev/tensorflow/albert_en_preprocess/2)
transforms plain text inputs into this format, which its documentation
describes in greater detail.

### Basic usage

The simplest way to use this model in the
[Keras functional API](https://www.tensorflow.org/guide/keras/functional)
is

```python
text_input = tf.keras.layers.Input(shape=(), dtype=tf.string)
preprocessor = hub.KerasLayer(
    "http://tfhub.dev/tensorflow/albert_en_preprocess/2")
encoder_inputs = preprocessor(text_input)
encoder = hub.KerasLayer(
    "https://tfhub.dev/tensorflow/albert_en_xlarge/2",
    trainable=True)
outputs = encoder(encoder_inputs)
pooled_output = outputs["pooled_output"]      # [batch_size, 2048].
sequence_output = outputs["sequence_output"]  # [batch_size, seq_length, 2048].
```

The encoder's outputs are the `pooled_output` to represents each input sequence
as a whole, and the `sequence_output` to represent each input token in context.
Either of those can be used as input to further model building.

To print pooled_outputs for inspection, the following code can be used:

```python
embedding_model = tf.keras.Model(text_input, pooled_output)
sentences = tf.constant(["(your text here)"])
print(embedding_model(sentences))
```

### Advanced topics

The [preprocessor documentation](http://tfhub.dev/tensorflow/albert_en_preprocess/2)
explains how to input segment pairs and how to control `seq_length`.

The intermediate activations of all L=24
Transformer blocks (hidden layers) are returned as a Python list:
`outputs["encoder_outputs"][i]` is a Tensor
of shape `[batch_size, seq_length, 2048]`
with the outputs of the i-th Transformer block, for `0 <= i < L`.
The last value of the list is equal to `sequence_output`.

The preprocessor can be run from inside a callable passed to
`tf.data.Dataset.map()` while this encoder stays a part of a larger
model that gets trained on that dataset.
<!--- TODO(b/171934083): Link to an example Colab. --->
The Keras input objects for running on preprocessed inputs are

```python
encoder_inputs = dict(
    input_word_ids=tf.keras.layers.Input(shape=(seq_length,), dtype=tf.int32),
    input_mask=tf.keras.layers.Input(shape=(seq_length,), dtype=tf.int32),
    input_type_ids=tf.keras.layers.Input(shape=(seq_length,), dtype=tf.int32),
)
```


## Changelog

### Version 2

  * Uses dicts (not lists) for inputs and outputs.
  * Comes with a companion model for preprocessing of plain text.
  * For legacy users, this version still provides the now-obsolete
    `albert_layer.resolved_object.sp_model_file` asset.

### Version 1

  * Initial release.

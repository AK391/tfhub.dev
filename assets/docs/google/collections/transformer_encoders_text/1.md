# Collection google/transformer_encoders_text/1

Transformer Encoders for Text

<!-- module-type: text-embedding -->
<!-- network-architecture: Transformer -->

## Overview

TF Hub offers a variety of models that use the
[Transformer](https://arxiv.org/abs/1706.03762) architecture,
more specifically its encoder part, to compute dense vector representations
for natural language.

## Catalog

The following models implement a common API for text embeddings with
transformer encoders, including the ability to preprocess plain text
for input into the transformer encoder model.

  * [ALBERT](https://tfhub.dev/google/collections/albert/1)
  * [BERT](https://tfhub.dev/google/collections/bert/1), including Small BERT
  * [BERT Experts](https://tfhub.dev/google/collections/experts/bert/1)
  * [ELECTRA](https://tfhub.dev/google/collections/electra/1)
  * Lambert [[large](https://tfhub.dev/tensorflow/lambert_en_uncased_L-24_H-1024_A-16/1)]
  * Small BERT: see [BERT](https://tfhub.dev/google/collections/bert/1)
  * Talking Heads & Gated GELU:
    [[base](https://tfhub.dev/tensorflow/talkheads_ggelu_bert_en_base),
    [large](https://tfhub.dev/tensorflow/talkheads_ggelu_bert_en_large)]

# Collection google/experts/bert/1

Collection of BERT experts fine-tuned on different datasets.

<!-- fine-tunable: true -->
<!-- format: saved_model_2 -->
<!-- dataset: Wikipedia and BooksCorpus -->
<!-- dataset: MNLI -->
<!-- dataset: QNLI -->
<!-- dataset: QQP -->
<!-- dataset: SST-2 -->
<!-- dataset: MEDLINE/PubMed -->
<!-- dataset: SQuAD 2.0 -->
<!-- module-type: text-embedding -->
<!-- network-architecture: Transformer -->
<!-- language: en -->

## Overview

Starting from a pre-trained BERT model and fine-tuning on the downstream task
gives impressive results on many NLP tasks. One can further increase the
performance by starting from a BERT model that better aligns or transfers to the
task at hand, particularly when having a low number of downstream examples. For
example, one can use a BERT model that was trained on text from a similar domain
or by use a BERT model that was trained for a similar task.

This is a collection of such BERT "expert" models that were trained on a
diversity of datasets and tasks to improve performance on downstream tasks like
question answering, tasks that require natural language inference skills, NLP
tasks in the medical text domain, and more.

## Wikipedia and BooksCorpus Experts

These models were pre-trained on the English Wikpedia[1] & BooksCourpus[2]
datasets. The models are intended to be used for a **variety of English NLP
tasks**.

Model                                                                              | Description
---------------------------------------------------------------------------------- | -----------
[Base Wikipedia + BooksCorpus model](https://tfhub.dev/google/experts/bert/pubmed) | The model was trained in a self-supervised manner on the **Wikipedia + BooksCorpus** datasets.
[MNLI model](https://tfhub.dev/google/experts/bert/wiki_books/mnli)                | This model was initialized from the base Wikipedia + BooksCorpus BERT model and was fine-tuned on MNLI, a dataset for **multi-genre natural language inference**.
[QNLI model](https://tfhub.dev/google/experts/bert/wiki_books/qnli)                | This model was initialized from the base Wikipedia + BooksCorpus BERT model and was fine-tuned on QNLI, a dataset for **question-based natural language inference**.
[QQP model](https://tfhub.dev/google/experts/bert/wiki_books/qqp)                  | This model was initialized from the base Wikipedia + BooksCorpus BERT model and was fine-tuned on the Quora Question Pairs dataset (QQP), a dataset for the **semantic similarity of question pairs**.
[SST-2 model](https://tfhub.dev/google/experts/bert/wiki_books/sst2)               | This model was initialized from the base Wikipedia + BooksCorpus BERT model and was fine-tuned on the Stanford Sentiment Treebank (SST-2), a dataset for **sentiment analysis**.
[SQuAD 2.0 model](https://tfhub.dev/google/experts/bert/wiki_books/sst2)           | This model was initialized from the base Wikipedia + BooksCorpus BERT model and was fine-tuned on the SQuAD 2.0, a dataset for **question-answering**.

## PubMed Experts

These models were pre-trained on the MEDLINE/PubMed[3] corpus of biomedical and
life sciences literature abstracts. The models are intended to be used on
**medical or scientific text NLP tasks**.

Model                                                                           | Description
------------------------------------------------------------------------------- | -----------
[Base PubMed model](https://tfhub.dev/google/experts/bert/pubmed)               | The model was trained in a self-supervised manner on the **MEDLINE/Pubmed** corpus.
[PubMed + SQuAD 2.0 model](https://tfhub.dev/google/experts/bert/pubmed/squad2) | This model was initialized from the base PubMed model and was fine-tuned on SQuAD 2.0, a dataset for **question-answering**.

\[1]: [Wikipedia dataset](https://dumps.wikimedia.org)

\[2]: [BooksCorpus dataset](http://yknzhu.wixsite.com/mbweb)

\[3]:
[MEDLINE/PubMed dataset](https://www.nlm.nih.gov/databases/download/pubmed_medline.html)

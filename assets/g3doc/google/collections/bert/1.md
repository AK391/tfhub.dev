# Collection google/bert/1

Bidirectional Encoder Representations from Transformers (BERT).

<!-- dataset: Wikipedia and BooksCorpus -->
<!-- module-type: text-embedding -->
<!-- network-architecture: Transformer -->
<!-- language: en -->

## Overview

The following are two sets of BERT models on tfhub.dev, in the form of
[TF2 SavedModels](https://www.tensorflow.org/hub/tf2_saved_model) and
[TF1 hub.Modules](https://www.tensorflow.org/hub/tf1_hub_module) and
respectively.

## TF2 SavedModel collection

These models use the implementation of BERT from the TensorFlow Models
repository on GitHub at
[tensorflow/models/official/nlp/bert](https://github.com/tensorflow/models/tree/master/official/nlp/bert).
See how to fine-tune these models with this
[Colab tutorial](https://colab.research.google.com/github/tensorflow/models/blob/master/official/colab/fine_tuning_bert.ipynb).

|            |
|------------|
| [tensorflow/bert_en_uncased_L-24_H-1024_A-16](https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16) |
| [tensorflow/bert_en_wwm_cased_L-24_H-1024_A-16](https://tfhub.dev/tensorflow/bert_en_wwm_cased_L-24_H-1024_A-16) |
| [tensorflow/bert_en_uncased_L-12_H-768_A-12](https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12) |
| [tensorflow/bert_en_wwm_uncased_L-24_H-1024_A-16](https://tfhub.dev/tensorflow/bert_en_wwm_uncased_L-24_H-1024_A-16) |
| [tensorflow/bert_en_cased_L-24_H-1024_A-16](https://tfhub.dev/tensorflow/bert_en_cased_L-24_H-1024_A-16) |
| [tensorflow/bert_en_cased_L-12_H-768_A-12](https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12) |
| [tensorflow/bert_zh_L-12_H-768_A-12](https://tfhub.dev/tensorflow/bert_zh_L-12_H-768_A-12) |
| [tensorflow/bert_multi_cased_L-12_H-768_A-12/1](https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12) |

All parameters in the modules are trainable, and fine-tuning all parameters is
the recommended practice.

## TF1 hub.Modules collection

These models use the original BERT implementation from
[github.com/google-research/bert](https://github.com/google-research/bert)
described in the paper "BERT: Pre-training of Deep Bidirectional Transformers
for Language Understanding" [1].

The models assume pre-processed inputs from the BERT repository
(https://github.com/google-research/bert)

All parameters in the module are trainable, and fine-tuning all parameters is
the recommended practice.

|            |
|------------|
| [google/bert_cased_L-12_H-768_A-12](https://tfhub.dev/google/bert_cased_L-12_H-768_A-12) |
| [google/bert_cased_L-24_H-1024_A-16](https://tfhub.dev/google/bert_cased_L-24_H-1024_A-16) |
| [google/bert_chinese_L-12_H-768_A-12](https://tfhub.dev/google/bert_chinese_L-12_H-768_A-12) |
| [google/bert_multi_cased_L-12_H-768_A-12](https://tfhub.dev/google/bert_multi_cased_L-12_H-768_A-12) |
| [google/bert_uncased_L-12_H-768_A-12](https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12) |
| [google/bert_uncased_L-24_H-1024_A-16](https://tfhub.dev/google/bert_uncased_L-24_H-1024_A-16) |

Additionally, there are the "Small BERT" models (English, uncased) described in
the paper "Well-Read Students Learn Better" [2] built from the same codebase.

|   |H=128|H=256|H=512|H=768|
|---|:---:|:---:|:---:|:---:|
| **L=2**  | [2/128](https://tfhub.dev/google/small_bert/bert_uncased_L-2_H-128_A-2/1)  | [2/256](https://tfhub.dev/google/small_bert/bert_uncased_L-2_H-256_A-4/1)  | [2/512](https://tfhub.dev/google/small_bert/bert_uncased_L-2_H-512_A-8/1)   | [2/768](https://tfhub.dev/google/small_bert/bert_uncased_L-2_H-768_A-12/1)   |
| **L=4**  | [4/128](https://tfhub.dev/google/small_bert/bert_uncased_L-4_H-128_A-2/1)  | [4/256](https://tfhub.dev/google/small_bert/bert_uncased_L-4_H-256_A-4/1)  | [4/512](https://tfhub.dev/google/small_bert/bert_uncased_L-4_H-512_A-8/1)   | [4/768](https://tfhub.dev/google/small_bert/bert_uncased_L-4_H-768_A-12/1)   |
| **L=6**  | [6/128](https://tfhub.dev/google/small_bert/bert_uncased_L-6_H-128_A-2/1)  | [6/256](https://tfhub.dev/google/small_bert/bert_uncased_L-6_H-256_A-4/1)  | [6/512](https://tfhub.dev/google/small_bert/bert_uncased_L-6_H-512_A-8/1)   | [6/768](https://tfhub.dev/google/small_bert/bert_uncased_L-6_H-768_A-12/1)   |
| **L=8**  | [8/128](https://tfhub.dev/google/small_bert/bert_uncased_L-8_H-128_A-2/1)  | [8/256](https://tfhub.dev/google/small_bert/bert_uncased_L-8_H-256_A-4/1)  | [8/512](https://tfhub.dev/google/small_bert/bert_uncased_L-8_H-512_A-8/1)   | [8/768](https://tfhub.dev/google/small_bert/bert_uncased_L-8_H-768_A-12/1)   |
| **L=10** | [10/128](https://tfhub.dev/google/small_bert/bert_uncased_L-10_H-128_A-2/1)| [10/256](https://tfhub.dev/google/small_bert/bert_uncased_L-10_H-256_A-4/1)| [10/512](https://tfhub.dev/google/small_bert/bert_uncased_L-10_H-512_A-8/1) | [10/768](https://tfhub.dev/google/small_bert/bert_uncased_L-10_H-768_A-12/1) |
| **L=12** | [12/128](https://tfhub.dev/google/small_bert/bert_uncased_L-12_H-128_A-2/1)| [12/256](https://tfhub.dev/google/small_bert/bert_uncased_L-12_H-256_A-4/1)| [12/512](https://tfhub.dev/google/small_bert/bert_uncased_L-12_H-512_A-8/1) | [12/768](https://tfhub.dev/google/small_bert/bert_uncased_L-12_H-768_A-12/1) |

#### References

[1] Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova. [BERT:
Pre-training of Deep Bidirectional Transformers for Language
Understanding](https://arxiv.org/abs/1810.04805). arXiv preprint
arXiv:1810.04805, 2018.

[2] Iulia Turc, Ming-Wei Chang, Kenton Lee, Kristina Toutanova. [Well-Read
Students Learn Better: On the Importance of Pre-training Compact
Models](https://arxiv.org/abs/1908.08962). arXiv preprint arXiv:1908.08962,
2019.
